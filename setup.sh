#!/bin/bash
# OLOL Quick Setup Script

echo "ðŸš€ OLOL Quick Setup"
echo "=================="

# Check Python
echo "ðŸ“‹ Checking Python..."
if ! command -v python3 &> /dev/null; then
    echo "âŒ Python 3 not found. Please install Python 3.8+ first."
    exit 1
fi
python3 --version

# Check Ollama
echo "ðŸ“‹ Checking Ollama..."
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama not found. Installing..."
    curl -fsSL https://ollama.ai/install.sh | sh
fi
ollama --version

# Install Python dependencies
echo "ðŸ“¦ Installing Python dependencies..."
python3 -m pip install -r requirements.txt

# Install PyTorch with GPU support
echo "ðŸ”¥ Installing PyTorch with CUDA support..."
# For RTX 5090 (Blackwell), use nightly builds with CUDA 12.8
if lspci | grep -i "rtx 5090\|rtx 5080\|rtx 5070" > /dev/null 2>&1; then
    echo "   ðŸŽ¯ RTX 50 series detected - installing nightly PyTorch with CUDA 12.8"
    python3 -m pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
else
    echo "   ðŸ”§ Using stable PyTorch with CUDA 12.4"
    python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
fi

# Install OLOL package
echo "ðŸ“¦ Installing OLOL..."
python3 -m pip install -e .

# Check if models exist
echo "ðŸ“‹ Checking models..."
MODEL_COUNT=$(ollama list | wc -l)
if [ $MODEL_COUNT -le 1 ]; then
    echo "ðŸ“¥ No models found. Downloading a test model..."
    ollama pull llama3.2:3b
fi

# Test installation
echo "ðŸ§ª Testing installation..."
python3 -c "from src.olol.proto import ollama_pb2; print('âœ… OLOL imports working')"

echo ""
echo "âœ… Setup complete!"
echo ""
echo "ðŸš€ To start OLOL:"
echo "  Terminal 1: python3 -m olol server"
echo "  Terminal 2: python3 -m olol proxy --servers localhost:50051"
echo ""
echo "ðŸ“– For detailed setup, see DEPLOYMENT_GUIDE.md"